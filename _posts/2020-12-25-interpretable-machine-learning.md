---
title: 机器学习的可解释性
tags:
  - 可解释机器学习
  - 机器学习
  - 可解释性
  - 黑盒子
  - 模仿者模型
---

1950年，世界伟大的计算机学家阿兰·图灵提出著名的“图灵测试”，为人工智能指引了发展方向。历经70余年的发展，人工智能技术取得长足发展与进步，在人们日常生活与工业自动化领域发挥着重要作用。

人工智能技术的发展经历了从“符号智能”到“感知智能”再到“认知智能”的发展阶段，目前感知智能已基本实现，现阶段正处于感知智能向认知智能跨越的阶段。这其中，机器学习技术无疑是推动技术演变的重要角色。特别是深度学习技术，为感知智能奠定了基础，相信在认知智能阶段也离不开深度学习技术的支撑。然而，当前深度学习技术依然依赖于数据驱动，是一个黑盒模型。为探索模型内部的学习过程，了解机器学习模型到底是如何学习的，相关学者开始研究机器学习的可解释性。

中国人民大学孟小峰在《计算机研究与发展》上刊文“机器学习的可解释性”，通过对现有的可解释方法进行归类描述和分析比较，一方面对可解释性的定义、度量进行阐述，另一方面针对可解释对象的不同，从模型的解释、预测结果的解释和模仿者模型的解释３个方面，总结和分析各种机器学习可解释技术，并讨论了机器学习可解释方法面临的挑战和机遇以及未来的可能发展方向。

纵观机器学习的历史发展进程，其最初的目标是从一系列数据中寻找出可以解释的知识，因而在追求算法性能的同时，也很注重算法的可解释性。典型的代表譬如线性感知机、决策树、k 近邻算法等。进入20世纪80年代之后，伴随神经网络的复苏，机器学习算法在设计时开始放弃可解释性这一要求，强调提高算法泛化的性能。神经网络的激活函数的选择不再局限于线性函数，而采用非线性的譬如Sigmoid，tanh，Softmax，Relu等函数，一方面其表示能力大幅度提高，另一方面，随着其模型复杂度的增加，算法的可解释性就更差。

机器学习解释技术具有巨大的潜在应用空间。譬如科学家在知识发现的过程中，可解释的机器学习系统可以帮助他们更好地理解输出的知识，并寻找各种因素之间的相关性；对于一些复杂任务的端到端系统，几乎无法完全测试，也无法创建系统可能失败的完整场景列表，人类无法枚举出所有可能出现的计算上或者逻辑上的不可行输出，系统的可解释性对于系统的理解则至关重要；需要防范可能产生某些歧视的场景，即使我们有意识将某些特定的受保护类编码到系统中，也仍然存在考虑欠缺的先验偏见，譬如种族歧视、性别歧视等。

对机器学习的可解释性需求在某些领域是不需要的，如如邮政编码分类、航空器防撞系统等都是在没有人类干预的情况下运行，不需要解释。但是在医疗保健、金融等行业而言，模型的可解释性不仅重要而且非常必要。譬如在医疗保健方面，护理人员、医生和临床专家都依赖于新的医疗技术来帮助他们监控和决策患者护理，一个良好的可解释性模型被证明可以提高临床工作人员的解决问题的能力，从而提高患者护理质量。通常对于系统出现不可接受的结果且无法造成重大后果的情况下，或者在实际应用中，人们已经充分地研究和验证出现的问题，即使系统表现不太完美，人们也愿意相信系统的决定。在类似的场景下，对可解释性是没有需求的。

## 1.可解释定义
## 2.主要研究方向
## 3.机器学习的可解释研究框架