---
title: "张钹院士：迈向第三代人工智能"
tags:
  - 人工智能
---

清华大学人工智能研究院院长、中国科学院院士张钹教授在“纪念《中国科学》创刊70周年专刊”上发表署名文章，首次全面阐述第三代人工智能的理念，提出第三代人工智能的发展路径是融合第一代的知识驱动和第二代的数据驱动的人工智能, 同时利用知识、数据、算法和算力等4个要素, 建立新的可解释和鲁棒的AI理论与方法，发展安全，可信，可靠和可扩展的AI技术，这是发展AI的必经之路。

![Accounting Services](/assets/images/2/ssai-paper.png)

原文下载：[http://scis.scichina.com/cn/2020/SSI-2020-0204.pdf](http://scis.scichina.com/cn/2020/SSI-2020-0204.pdf)

人工智能（Artificial Intelligence，简称AI）在60多年的发展历史中，一直存在两个相互竞争的范式，即符号主义与连接主义（或称亚符号主义）。符号主义（即第一代人工智能）到上个世纪八十年代之前一直主导着AI的发展，而连接主义（即第二代人工智能）从上个世纪九十年代逐步发展，到本世纪初进入高潮，大有替代符号主义之势。但是今天看来，这两种范式只是从不同的侧面模拟人类的心智（或大脑），具有各自的片面性，不可能触及人类真正的智能。

## 1 第一代人工智能

人类的智能行为是怎么产生的, 纽威尔 (A. Newell)、西蒙 (H. A. Simon) 等 [1∼4] 提出以下模拟人类大脑的符号模型, 即物理符号系统假设. 这种系统包括:  
(1) 一组任意的符号集, 一组操作符号的规则集;  
(2) 这些操作是纯语法 (syntax) 的, 即只涉及符号的形式不涉及语义, 操作的内容包括符号的组合和重组;  
(3) 这些语法具有系统性的语义解释, 即它所指向的对象和所描述的事态.  

1955 年麦卡锡(J. McCarthy) 和明斯基 (M. L. Minsky) 等学者[5], 在达特茅斯人工智能夏季研究项目 (the Dartmouth Summer Research Project on Artificial Intelligence) 的建议中, 明确提出**符号 AI (artificial intelligence)的基本思路: “人类思维的很大一部分是按照推理和猜想规则对 ‘词’ (words) 进行操作所组成的”.** 根据这一思路, 他们提出了基于知识与经验的推理模型, 因此我们又把符号AI称为知识驱动方法.

符号AI的开创者最初把注意力放在研究推理(搜索)的通用方法上, 如“手段 – 目的分析” (meanend analysis)、 “分而治之” (divide and conquer)、 “试错” (trial and error) 法等, 试图通过通用的方法解决范围广泛的现实问题. 由于通用方法是一种弱方法, 实际上只能解决 “玩具世界” 中的简单问题,如机器人摆放积木, 下简单的井字棋(tic-tac-toe)等, 与解决复杂现实问题相差很远. 寻求通用AI的努力遭到了失败, 符号AI于20世纪70年代初跌入低谷.幸运的是,斯坦福大学教授费根堡姆(E.A.Feigenbaum)等及时改变了思路,认为知识,特别是特定领域的知识才是人类智能的基础,提出知识工程(knowledgeengineering)与专家系统(expertsystems)等一系列强AI方法,给符号AI带来了希望.他们开发了专家系统DENDRAL(有机化学结构分析系统,1965∼1975)[6],随后其他学者相继开发了MYCIN(血液传染病诊断和抗菌素处方,1971∼1977)[7],XCON(计算机硬件组合系统)等.不过早期的专家系统规模都较小,难以实用.直到1997年5月IBM的深蓝(deepblue)国际象棋程序打败世界冠军卡斯帕诺夫(Kasparov),符号AI才真正解决大规模复杂系统的开发问题.费根堡姆和雷蒂(R.Raddy)作为设计与构造大型人工智能系统的先驱,共同获得1994年ACM图灵奖.

符号AI同样可以应用于机器学习,把“机器学习”看成是基于知识的(归纳)推理.下面以归纳逻辑编程(inductive logic programming,ILP)[8]为例说明符号AI的学习机制.在ILP中正负样本(具体示例)、背景知识和学习结果(假设)都以一阶逻辑子句(程序)形式表示.学习过程是在假设空间中寻找一个假设,这个假设应尽可能多地包含正例,尽量不包含负例,而且要与背景知识一致.一般情况下假设空间很大,学习十分困难,不过有了背景知识之后,就可以极大地限制假设空间,使学习变成可行.显然,背景知识越多,学习速度越快,效果也越好.为解决不确定问题,近年来,发展了概率归纳逻辑编程方法(probabilistic inductive logic programming,PILP)[9].基于知识的学习,由于有背景知识,可以实现小样本学习,而且也很容易推广到不同的领域,学习的鲁棒性也很强.以迁移学习(transferlearning)[10]为例,可以将学习得到的模型从一种场景更新或者迁移到另一场景,实现跨领域和跨任务的推广.具体做法如下,首先,从学习训练的环境(包括训练数据与方法)出发,发现哪些(即具有某种通用性)知识可以跨域或者跨任务进行迁移,哪些只是针对单个域或单个任务的特定知识,并利用通用知识帮助提升目标域或目标任务的性能.这些通用知识主要通过以下4种渠道迁移到目标域中去,即源域中可利用的实例,源域和目标域中可共享的特征,源域模型可利用的部分,源域中实体之间的特定规则.可见,知识在迁移学习中起关键的作用,因此,符号AI易于跨领域和跨任务推广.

在创建符号AI中做出重大贡献的学者中,除费根堡姆和雷蒂(1994)之外,还有明斯基(1969),麦卡锡(1971),纽威尔和西蒙(1975)共6位先后获得图灵奖(括号中的数字表示获奖的年份).总之,第一代AI的成功来自于以下3个基本要素.以深蓝程序为例,第1是知识与经验,“深蓝”从象棋大师已经下过的70万盘棋局和大量5∼6个棋子的残局中,总结出下棋的规则.另外,在象棋大师与深蓝对弈的过程中,通过调试“评价函数”中的6000个参数,把大师的经验引进程序.第2是算法,深蓝采用α−β剪枝算法,有效提高搜索效率.第3是算力(计算能力),为了达到实时的要求,深蓝使用IBMRS/6000SP2,11.38GFLOPS(浮点运算/秒),每秒可检查2亿步,或3分钟运行5千万盘棋局(positions).

符号AI有坚实的认知心理学基础,把符号系统作为人类高级心智活动的模型,其优势是,由于符号具有可组合性(compositionality),可从简单的原子符号组合成复杂的符号串.每个符号都对应着一定的语义,客观上反映了语义对象的可组合性,比如,由简单部件组合成整体等,可组合性是推理的基础,因此符号AI与人类理性智能一样具有可解释性和容易理解.符号AI也存在明显的局限性,目前已有的方法只能解决完全信息和结构化环境下的确定性问题,其中最具代表性的成果是IBM“深蓝”国际象棋程序,它只是在完全信息博弈(决策)中战胜人类,这是博弈中最简单的情况.而人类的认知行为(cognitive behavior),如决策等都是在信息不完全和非结构化环境下完成的,符号AI距离解决这类问题还很远.

以自然语言形式表示(离散符号)的人类知识,计算机难以处理,必须寻找计算机易于处理的表示形式,这就是知识表示问题.我们已有的知识表示方法,如产生式规则(production rules),逻辑程序(logic program)等,虽然计算机易于处理(如推理等),但都较简单,表现能力有限,难以刻画复杂和不确定的知识,推理也只限于逻辑推理等确定性的推理方法.更加复杂的知识表示与推理形式都在探讨之中,如知识图谱(knowledge graph)[11]、概率推理等[12].符号AI缺乏数学基础,除数理逻辑之外,其他数学工具很难使用,这也是符号AI难以在计算机上高效执行的重要原因.基于知识驱动的强AI只能就事论事地解决特定问题,有没有广泛适用的弱方法,即通用AI,目前还是一个值得探讨的问题.此外,从原始数据(包括文本、图像、语音和视频)中获取知识目前主要靠人工,效率很低,需要探索有效的自动获取方法.此外,真正的智能系统需要常识,常识如何获取、表达和推理还是一个有待解决的问题.常识的数量巨大,构造一个实用的常识库,无异于一项AI的“曼哈顿工程”,费时费力.

## 2 第二代人工智能

感官信息(视觉、听觉和触觉等)是如何存储在记忆中并影响人类行为的?有两种基本观点,一种观点是,这些信息以某种编码的方式表示在(记忆)神经网络中,符号AI属于这一学派.另一种观点是,感官的刺激并不存储在记忆中,而是在神经网络中建立起“刺激–响应”的连接(通道),通过这个“连接”保证智能行为的产生,这是连接主义的主张,连接主义AI就是建立在这个主张之上.1958年罗森布拉特(Rosenblatt)按照连接主义的思路,建立一个人工神经网络(artificial neural network,ANN)的雏形——感知机(perceptron)[13,14].感知机的灵感来自于两个方面,一是1943年麦卡洛克(McCulloch)和皮特(Pitts)提出的神经元数学模型——“阈值逻辑”线路,它将神经元的输入转换成离散值,通常称为M-P模型[15].二是来自于1949年赫布(D.O.Hebb)提出的Hebb学习率,即“同时发放的神经元连接在一起”[16].感知机如图1所示.

![感知机](/assets/images/2/perceptron.png)

![公式1](/assets/images/2/perceptron-equal.png)

其中b为阈值,w为权值.

AI的创建者从一开始就关注连接主义的思路.1955年麦卡锡等在达特茅斯(Dartmouth)AI研究建议中写道“如何安排一组(假想的)神经元使之形成概念······已经获得部分的结果,但问题是需要更多的理论工作”[5],并把它列为会议的研讨内容之一.由感知机组成的ANN只有一个隐蔽层,过于简单.明斯基等[17]于1969年出版的书《感知机》中指出,感知机只能解决线性可分问题,而且即使增加隐层的数量,由于没有有效的学习算法,感知机也很难实用.明斯基对感知机的批评是致命的,使刚刚起步的连接主义AI跌入低谷达10多年之久.在困难的时期里,在许多学者的共同努力下,30多年来无论在神经网络模型还是学习算法上均取得重大进步,逐步形成了深度学习的成熟理论与技术.其中重要的进展有,第1,梯度下降法(gradient descent),这本来是一个古老的算法,法国数学家柯西(Cauchy)[18]早在1847年就已经提出;到1983年俄国数学家尤里·涅斯捷诺夫(YuriiNesterov)[19]做了改进,提出了加强版,使它更加好用.第2,反向传播(back propagation,BP)算法,这是为ANN量身定制的,1970年由芬兰学生Seppo Linnainmaa在他的硕士论文中首先提出;1986年鲁梅哈特(D.E.Rumelhart)和辛顿(G.Hinton)等做了系统的分析与肯定[20].“梯度下降”和“BP”两个算法为ANN的学习训练注入新的动力,它们和“阈值逻辑”、“Hebb学习率”一起构成ANN的4大支柱.除4大支柱之外,还有一系列重要工作,其中包括更好的损失函数,如交叉熵损失函数(cross-entropy cost function)[21];算法的改进,如防止过拟合的正则化方法(regularization)[22];新的网络形式,如1980年日本福岛邦彦(Fukushima)的卷积神经网络(convolution neural networks,CNN)[23,24],递归神经网络(recurrent neural networks,RNN)[25],长短程记忆神经网络(long short-term memory neural networks,LSTM)[26],辛顿的深度信念网络(deep belief nets,DBN)[27]等.这些工作共同开启了以深度学习(deep learning)为基础的第二代AI的新纪元[28].

第二代AI的学习理论有坚实的数学基础,为了说明这个基础,下面举一个简单的有监督学习的例子,有监督学习可以形式化为以下的函数回归问题:从数据库D中提取样本(xi,yi)i←−:i:d:(X,Y),对样本所反映的输入–输出关系f:X→Y做出估计,即从备选函数族(假设空间)F={fθ:X−→Y;θ∈A}中选出一个函数f∗使它平均逼近于真实f.在深度学习中这个备选函数族由深度神经网络表示:

![公式2](/assets/images/2/equal2.png)

参数学习中有3项基本假设.(1)独立性假设:损失函数和备选函数族F(或者神经网络结构)的选择与数据无关.(2)大容量假设:样本(xi,yi)数量巨大(n→∞).(3)完备性假设:训练样本完备且无噪声.如果上述假设均能满足,f∗将随样本数的增加最后收敛于真实函数f.由此可见,如果拥有一定质量的大数据,由于深度神经网络的通用性(universality),它可以逼近任意的函数,因此利用深度学习找到数据背后的函数具有理论的保证.这个论断在许多实际应用中得到了印证,比如,在标准图像库ImageNet(2万类别,1千4百万张图片)上的机器识别性能,2011年误识率高达50%,到2015年微软公司利用深度学习方法,误识率大幅度地降到3.57%,比人类的误识率5.1%还要低[29].低噪声背景下的语音识别率,2001年之前基本上停留在80%左右,到了2017年识别率达到95%以上,满足商品化的要求.2016年3月谷歌围棋程序AlphaGo打败世界冠军李世石,是第二代AI巅峰之作,因为在2015年之前计算机围棋程序最高只达到业余五段!更加令人惊奇的是,这些超越人类性能成果的取得,并不需要领域知识的帮助,只需输入图像原始像素、语音原始波形和围棋棋盘的布局(图像)!深度学习的成功来自于以下3个要素,一是数据,以AlphaGo为例,其中AlphaGo-Zero通过强化学习自学了亿级的棋局,而人类在千年的围棋史中,下过的有效棋局只不过3000万盘.二是算法,包括蒙特卡洛树搜索(Monte-Carlo tree search)[30],深度学习和强化学习(reinforcement learning)[31]等.三是算力,运行AlphaGo的机器是由1920个CPU和280个GPU组成的分布系统.因此第二代AI又称数据驱动方法.

在创建第二代AI中做出重大贡献的学者中,有以下5位获得图灵奖.他们是菲丽恩特(L.G.Valiant,2010)、珀尔(J.Pearl,2011)、本杰奥(Y.Bengio,2018)、辛顿(G.Hinton,2018)、杨立昆(Y.LeCun,2018)等.

早在2014年,深度学习的诸多缺陷不断地被发现,预示着这条道路遇到了瓶颈.下面仅以基于深度学习的图像识别的一个例子说明这个问题(材料引自本团队的工作).文献[32]表示利用基于动量的迭代快速梯度符号法(momentum iterative fast gradient sign method,MI-FGSM)对Inception v3深度网络模型实施攻击的结果.无噪声的原始图像——阿尔卑斯山(Alps),模型以94.39%的置信度得到正确的分类.利用MI-FGSM方法经10次迭代之后生成攻击噪声,将此攻击噪声加进原图像后得到攻击样本.由于加入的噪声很小,生成的攻击样本与原始图几乎没有差异,人类无法察觉,但Inception v3模型却以99.99%的置信度识别为“狗”.

深度学习为何如此脆弱,这样容易受攻击,被欺骗和不安全.原因只能从机器学习理论本身去寻找.机器学习的成功与否与3项假设密切相关,由于观察与测量数据的不确定性,所获取的数据一定不完备和含有噪声,这种情况下,神经网络结构(备选函数族)的选择极为重要,如果网络过于简单,则存在欠拟合(under-fitting)风险,如果网络结构过于复杂,则出现过拟合(overfitting)现象.虽然通过各种正则化的手段,一定程度上可以降低过拟合的风险,但是如果数据的质量差,则必然会导致推广能力的严重下降.此外,深度学习的“黑箱”性质是造成深度学习推广能力差的另一个原因,以图像识别为例,通过深度学习只能发现重复出现的局部片段(模式),很难发现具有语义的部件.文献[33]描述了利用深度网络模型VGG-16对“鸟”原始图像进行分类,从该模型pool5层147号神经元的响应可以看出,该神经元最强烈的响应是“鸟”头部的某个局部特征,机器正利用这个局部特征作为区分“鸟”的主要依据,显然它不是“鸟”的不变语义特征.因此对于语义完全不同的对抗样本(人物、啤酒瓶和马等),由于具有与“鸟”头部相似的片段,VGG-16模型pool5层147号神经元同样产生强烈的响应,于是机器就把这些对抗样本错误地判断为“鸟”.