---
title: 超大语言模型少样本学习
tags:
  - 少样本学习
  - 语言模型
---

最近的工作表明，通过对大量文本语料库进行预训练，然后对特定任务进行微调，在许多NLP任务和基准测试方面取得了实质性进展。虽然这种方法在架构中通常与任务无关，但它仍然需要成千上万个样例的特定于任务的微调数据集。相比之下，人类通常只通过几个例子或简单的指令就能完成一项新的语言任务——这是目前的NLP系统在很大程度上难以做到的。我将讨论GPT-3，这是一种具有1750亿个参数的自回归语言模型，它演示了如何扩大语言模型可以极大地改善与任务无关的、少样本的性能，有时甚至可以达到与先前的最先进的微调方法相媲美的竞争力。GPT-3可以应用于没有任何渐变更新或微调的任务，与少数样本演示指定纯粹通过文本与模型的交互。我将概述GPT-3是什么以及它是如何工作的，讨论我们从这样一个系统中看到的功能，以及它们如何启用与语言模型交互的新方式，此外还将关注这些交互带来的局限性和更广泛的问题。